# ğŸ“˜ Mentor-Scoring-AI

### *AI-Driven Evaluation of Teaching Quality from Recorded Video Sessions*

**Submission for:** *UpSkill India Challenge â€” Techfest IIT Bombay x HCL GUVI*

---

## ğŸš€ Overview

Evaluating teaching quality across large education ecosystems is slow, subjective, and inconsistent.
**Mentor-Scoring-AI** is an **AI-powered multimodal assessment system** that analyzes video lectures and automatically scores a mentor on:

* Communication clarity
* Engagement & gestures
* Technical depth
* Confidence & pacing
* Interaction quality

All using **audio, visual pose analysis, and transcript intelligence**.

Built as a **lightweight, on-device, offline-capable prototype**, our solution focuses on **scalability, objectivity, and automation**â€”precisely addressing the Problem Statement #2 of the hackathon.

---

## ğŸ§  Key Features

### ğŸ”Š **Audio Intelligence (Whisper + faster-whisper)**

* Clean audio extraction (FFmpeg)
* Fast speech-to-text transcription
* Sentence & vocabulary-based clarity scoring

### ğŸ‘ï¸ **Visual + Gesture Intelligence (YOLOv11 Pose)**

* Hand movement analysis â†’ engagement score
* Face visibility & eye-contact tracking â†’ confidence score
* Lightweight YOLOv11n â†’ **5Ã— faster** processing

### ğŸ“„ **Concept Depth & Explanation Analysis**

* Ollama Llama3 local inference (if available)
* Smart fallback heuristic scoring
* Segment-wise depth metrics

### ğŸ›ï¸ **Unified Streamlit Dashboard**

* Upload or URL-based analysis
* Instant breakdown of all metrics
* Tab-wise transcript & gesture insights
* 1-click evaluation report

---

## ğŸ¯ Why This Matters

Institutions often deal with:

* Highly variable mentor performance
* Lack of standardized evaluation
* Manual review overhead
* Difficulty scaling quality checks

**Our system solves this by offering:**
âœ” Consistent, unbiased scoring
âœ” Automated evaluation â€” scalable to 1,000+ videos
âœ” Actionable insights for teacher improvement
âœ” Offline/on-device capability â†’ low-cost deployment
âœ” Multimodal analysis like real human evaluators

---

## ğŸ—ï¸ System Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚       Video Input         â”‚
                    â”‚ (Upload or URL Download)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                 â”‚                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Audio Extract  â”‚ â”‚   Video Frames   â”‚ â”‚ Transcript Engine â”‚
        â”‚   (FFmpeg)      â”‚ â”‚  Sampling (cv2)  â”‚ â”‚ (Whisper/Faster)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                 â”‚                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Speech-to-Text  â”‚ â”‚   YOLO Pose      â”‚ â”‚ Concept Depth AI â”‚
        â”‚ Whisper Model   â”‚ â”‚ Hand/Eye/Face    â”‚ â”‚ (Ollama / local) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚   Detection       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                  â”‚                  â”‚                    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼                 â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     Scoring & Aggregation      â”‚
                     â”‚ (Engagement, Clarity, Depth)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  Streamlit Report   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Repository Structure

```
Mentor-Scoring-AI/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ system_design.md
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ evaluation_notes.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ transcribe.py
â”‚   â”‚   â”œâ”€â”€ gesture_analysis.py
â”‚   â”‚   â””â”€â”€ depth_analysis.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ ffmpeg_utils.py
â””â”€â”€ models/
    â””â”€â”€ model_download_links.txt
```

---

## âš™ï¸ Tech Stack

### **AI Models**

* Whisper + faster-whisper (speech-to-text)
* YOLOv11n-Pose (gesture + visual cues)
* Llama3 (Ollama) or heuristic fallback (depth scoring)

### **Core Libraries**

* OpenCV
* MoviePy
* YOLO (Ultralytics)
* FFmpeg
* FastAPI (for model structure)
* Streamlit (demo UI)

---

## ğŸ§ª Evaluation Metrics

The judge-provided metric distribution is *natively integrated* into our scoring:

| Skill Metric        | Weight | Data Source           |
| ------------------- | ------ | --------------------- |
| Engagement          | 20%    | YOLO Pose (gestures)  |
| Communication       | 20%    | Whisper transcript    |
| Technical Depth     | 30%    | Depth Analysis (LLM)  |
| Clarity             | 20%    | Transcript complexity |
| Interaction Quality | 10%    | Eye contact + pacing  |

---

## ğŸ› ï¸ Quick Start (Development)

### 1ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate (Windows)
pip install -r requirements.txt
```

### 2ï¸âƒ£ FFmpeg configuration

Update path in:
`src/utils/ffmpeg_utils.py`
OR ensure ffmpeg is in PATH.

### 3ï¸âƒ£ Run Streamlit demo

```bash
streamlit run src/app.py
```

### 4ï¸âƒ£ Upload video â†’ Get full AI report

Accepted formats: `.mp4`, `.mov`, `.avi`.

---

## ğŸ¥ Demo Output (What Judges Will See)

### âœ” Transcript Summary

* Word count
* Sentence clarity
* Key concepts detected
* Complexity measure

### âœ” Gesture & Engagement Analysis

* Hand movement intensity
* Eye contact %, face visibility
* Confidence cues
* 25-frame sampled evaluation

### âœ” Depth Score & Reasoning

* JSON-based segment evaluation
* LLM reasoning text
* Overall depth score (0â€“1)

### âœ” Final â€œMentor Scoreâ€

Weighted composite score aligned with hackathon criteria.

---

## ğŸ“ˆ Innovation & Differentiators

ğŸ”¥ **5Ã— faster** multimodal processing (YOLO11n + optimized sampling)
ğŸ”¥ Local + cloud-free analysis (Ollama fallback)
ğŸ”¥ Multi-segment depth scoring
ğŸ”¥ Built with low compute footprint (runs on CPU)
ğŸ”¥ URL-based YouTube lecture evaluation
ğŸ”¥ Production-ready modular architecture

---

## ğŸ§­ Roadmap (Post-Hackathon)

* Add **bias-free scoring calibration**
* Introduce **Live Mentor Evaluation** (real-time camera)
* Mentor benchmarking dashboard
* Session comparison & trend analytics
* Institution-wide scoring API

---



## Things Left in Implementation (Current Status)

The following components are planned and partially implemented but not fully integrated:

1. **Database and Dashboard Integration**

   * Dashboard UI structure is built
   * Database linking/connection logic is pending

2. **Database Connection With Main Website**

   * Backendâ€“DB binding still needs to be implemented
   * Intended for storing mentor scores, video metadata, and analytics

These features will complete the systemâ€™s ability to store evaluations, visualize historical insights, and integrate end-to-end with a centralized platform.




## ğŸ‘¥ Team

**Abhishek Boyane**
**Ishan Kalhe**
**Yash Bhosale**
**Chetan Patel**
Roles include:

* AI/ML(**Abhishek Boyane**)
* Backend
* Vision Processing(**Yash Bhosale**)
* Full-stack(**Ishan Kalhe**)
* UI/UX(**Chetan Patel**)

---

## ğŸ“© Contact

**Email:ishankalhe1@gmail.com** 


